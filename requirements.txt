# FastAPI and Server
fastapi==0.104.1
uvicorn[standard]==0.24.0
python-multipart==0.0.6
starlette==0.27.0  # FastAPI dependency

# Data Validation
pydantic>=2.5.0
python-dotenv==1.0.0

# NLP Libraries - Core
transformers>=4.35.0
tokenizers>=0.14.0  # Pre-built wheels, no Rust needed
# Use CPU-only torch to reduce image size
--extra-index-url https://download.pytorch.org/whl/cpu
torch>=2.2.0
sentencepiece>=0.1.99  # Required for XLM-RoBERTa

# Text Processing
nltk==3.8.1
spacy>=3.7.0  # Fast tokenization and preprocessing
textblob==0.17.1

# Language Detection
fasttext-wheel>=0.9.2  # Pre-compiled FastText (176 languages)
langdetect>=1.0.9  # Fallback language detector

# Data Processing
pandas==2.1.3
numpy>=1.26.0,<3.0.0  # Compatible with both fasttext and transformers

# Caching
redis>=5.0.0

# Testing
pytest==7.4.3
pytest-cov==4.1.0
pytest-asyncio==0.21.1
httpx==0.25.1

# Code Quality
flake8==6.1.0
black==23.11.0

# Optional Performance Enhancements
# hf_xet  # For faster HuggingFace downloads (optional)

# Model-specific dependencies
# Note: Models will be loaded from Hugging Face Hub:
# - l3cube-pune/hing-bert (440 MB) - Hinglish token-level detection
# - l3cube-pune/hing-roberta (500 MB) - Hinglish sentiment
# - cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual (560 MB) - Multilingual sentiment
# - en_core_web_sm (13 MB) - spaCy English model

